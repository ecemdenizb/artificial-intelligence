{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hurriyet-ds.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOjicmKi9HbHJbaSWrqpayp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ecemdenizb/artificial-intelligence/blob/main/hurriyet_ds.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWUCZrBBmpuT"
      },
      "source": [
        "import io\n",
        "import itertools\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import tensorflow as tf\n",
        "import tqdm\n",
        "\n",
        "from tensorflow.keras import Model, Sequential\n",
        "from tensorflow.keras.layers import Activation, Dense, Dot, Embedding, Flatten, GlobalAveragePooling1D, Reshape\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cl-yygNrmxly"
      },
      "source": [
        "SEED = 42 \n",
        "AUTOTUNE = tf.data.AUTOTUNE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9HZRh_UmxjL",
        "outputId": "0cc6af25-0428-4f8f-b0dc-18ef4700092b"
      },
      "source": [
        "path_to_file = tf.keras.utils.get_file('hurriyet.txt', 'https://www.dropbox.com/s/zixgykc93s32bf7/hurriyet.txt?dl=1')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.dropbox.com/s/zixgykc93s32bf7/hurriyet.txt?dl=1\n",
            "56737792/56735350 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QiDAbW5OmxeZ",
        "outputId": "1c24fe5d-a393-413d-9114-77af1a1ae9e2"
      },
      "source": [
        "with open(path_to_file) as f: \n",
        "  lines = f.read().splitlines()\n",
        "for line in lines[:20]:\n",
        "  print(line)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iran devlet televizyonu ülkedeki eyaletin sinde yapılan reformcuları protesto amaçlı yürüyüşlere milyonlarca kişinin katıldığını bildirdi \n",
            "gösterilerde fitnecilere ölüm münafıklara ölüm abd ye ölüm ingiltere ye ölüm sloganları atıldı \n",
            "dini lider ali hamaney ve cumhurbaşkanı mahmud ahmedinejad ı destekleyen iranlılar son olaylarda yeğeni öldürülen mir hüseyin musevi başta olmak üzere muhalefet liderlerini kınadılar \n",
            "musevi ye ölüm ve idam idam sloganları duyuldu \n",
            "muhalefet liderleri kaçtı mı aşure günü yaşanan çatışmalarda devlet kaynaklarına göre u terörist olmak üzere kişi ölmüştü \n",
            "den fazla kişinin yaralandığı olaylar sırasında en az kişi tutuklanmıştı \n",
            "öte yandan iran haber ajansı irna muhalif liderler mir hüseyin musevi ve mehdi kerrubi nin başkentten kaçarak ülkenin kuzeyine geçtiğini ileri sürdü ancak muhalefet iddiayı yalanladı \n",
            "hamaney in bir dönem korumalığını yapan ve şu anda fransa da saklandığı söylenen bir kişinin muhalefete verdiği bilgilere göre münzevi yaşamı na rağmen dini liderin havyara karşı korkunç bir iştahı var \n",
            "baston ve at meraklısı hamaney aynı zamanda değerli mücevherlerle bezenmiş bastonların ve cins atların koleksiyonunu yapıyor \n",
            "hamaney in antika bastonlarının sayısı \n",
            "fanatik binicilik tutkunu olan hamaney den fazla cins ata sahip \n",
            "görkemli cübbesi ise özel olarak yetiştirilen develerin tüyünden dokunuyor \n",
            "anlatılanlar arasında en ilginci ali hamaney in sık sık geçirdiği depresyon nöbetlerini orta kademe bir din adamına müstehcen fıkralar anlattırarak atlattığı iddiası \n",
            "eski korumanın sözlerinin doğruluğuna ise hamaney in afyon kullandığı yolundaki söylentileri yalanlaması kanıt gösteriliyor \n",
            "yine iran dan kaçan istihbarat görevlisinin iddiaları da benzer yönde \n",
            "bu iddalara göre hamaney in avrupalı fabrikatörlerle afrikalı cep telefonu şirketleri ve uluslararası emtia pazarlarıyla iş bağlantıları var \n",
            "iranlı kaçak istihbaratçılar ayrıca hamaney e tahsis edilen saray olduğunu ve bunların nükleer saldırıya dayanıklı sığınaklara hatta tam teşekküllü hastaneye sahip olduklarını da öne sürdü \n",
            "işte abd yi korkutan minibüs araç kimlik numarası okunmayan gösterge paneli üzerinde sahte olduğu belirlenen üzerinde ˜görevli polis aracı ™ yazısı bulunan camları tente ile kapatılmış minibüs new york polisinin alarma geçmesine neden oldu \n",
            "yetkililer ayrıca broadway ile \n",
            "sokağın kesiştiği noktada park edilen şüpheli aracın birkaç gündür aynı yerde bulunduğunu belirtti \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4Gje9AKmxbx"
      },
      "source": [
        "text_ds = tf.data.TextLineDataset(path_to_file).filter(lambda x: tf.cast(tf.strings.length(x), bool))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tnXPsOknMl3"
      },
      "source": [
        "# We create a custom standardization function to lowercase the text and \n",
        "# remove punctuation.\n",
        "def custom_standardization(input_data):\n",
        "  lowercase = tf.strings.lower(input_data)\n",
        "  return tf.strings.regex_replace(lowercase,\n",
        "                                  '[%s]' % re.escape(string.punctuation), '')\n",
        "\n",
        "# Define the vocabulary size and number of words in a sequence.\n",
        "vocab_size = 4096\n",
        "sequence_length = 10\n",
        "\n",
        "# Use the text vectorization layer to normalize, split, and map strings to\n",
        "# integers. Set output_sequence_length length to pad all samples to same length.\n",
        "vectorize_layer = TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z06SyizknMjU"
      },
      "source": [
        "vectorize_layer.adapt(text_ds.batch(1024))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPXlhdbxnMg-",
        "outputId": "07800c60-d97a-4e6d-edd5-7c73abeb1106"
      },
      "source": [
        "# Save the created vocabulary for reference.\n",
        "inverse_vocab = vectorize_layer.get_vocabulary()\n",
        "print(inverse_vocab[:20])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['', '[UNK]', 've', 'bir', 'da', 'de', 'bu', 'nin', 'için', 'türkiye', 'ile', 'ın', 'in', 'nın', 'olarak', 'abd', 'daha', 'ise', 'olduğunu', 'çok']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afqImWWEnMSI"
      },
      "source": [
        "def vectorize_text(text):\n",
        "  text = tf.expand_dims(text, -1)\n",
        "  return tf.squeeze(vectorize_layer(text))\n",
        "\n",
        "# Vectorize the data in text_ds.\n",
        "text_vector_ds = text_ds.batch(1024).prefetch(AUTOTUNE).map(vectorize_layer).unbatch()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxAjF996nMOJ",
        "outputId": "4286fdfe-183e-4ad5-ea91-b2a90cbb6233"
      },
      "source": [
        "sequences = list(text_vector_ds.as_numpy_iterator())\n",
        "print(len(sequences))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "411525\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h033tgQOnwhN",
        "outputId": "ad079ea9-9308-4365-ed3a-9fb243cb1f46"
      },
      "source": [
        "for seq in sequences[:5]:\n",
        "  print(f\"{seq} => {[inverse_vocab[i] for i in seq]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[  66   81 1305  777    1    1   56    1  497 1726] => ['iran', 'devlet', 'televizyonu', 'ülkedeki', '[UNK]', '[UNK]', 'yapılan', '[UNK]', 'protesto', 'amaçlı']\n",
            "[3749    1  985    1  985   15   28  985  135   28] => ['gösterilerde', '[UNK]', 'ölüm', '[UNK]', 'ölüm', 'abd', 'ye', 'ölüm', 'ingiltere', 'ye']\n",
            "[ 707  675  527    1    2   84 1642 1303  152 2968] => ['dini', 'lider', 'ali', '[UNK]', 've', 'cumhurbaşkanı', 'mahmud', 'ahmedinejad', 'ı', 'destekleyen']\n",
            "[  1  28 985   2 673 673   1   1   0   0] => ['[UNK]', 'ye', 'ölüm', 've', 'idam', 'idam', '[UNK]', '[UNK]', '', '']\n",
            "[ 814 1670    1  501    1  139  289 1419   81 3940] => ['muhalefet', 'liderleri', '[UNK]', 'mı', '[UNK]', 'günü', 'yaşanan', 'çatışmalarda', 'devlet', 'kaynaklarına']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhBY-jYZoRVo"
      },
      "source": [
        "# Generates skip-gram pairs with negative sampling for a list of sequences\n",
        "# (int-encoded sentences) based on window size, number of negative samples\n",
        "# and vocabulary size.\n",
        "def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\n",
        "  # Elements of each training example are appended to these lists.\n",
        "  targets, contexts, labels = [], [], []\n",
        "\n",
        "  # Build the sampling table for vocab_size tokens.\n",
        "  sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
        "\n",
        "  # Iterate over all sequences (sentences) in dataset.\n",
        "  for sequence in tqdm.tqdm(sequences):\n",
        "\n",
        "    # Generate positive skip-gram pairs for a sequence (sentence).\n",
        "    positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
        "          sequence, \n",
        "          vocabulary_size=vocab_size,\n",
        "          sampling_table=sampling_table,\n",
        "          window_size=window_size,\n",
        "          negative_samples=0)\n",
        "\n",
        "    # Iterate over each positive skip-gram pair to produce training examples \n",
        "    # with positive context word and negative samples.\n",
        "    for target_word, context_word in positive_skip_grams:\n",
        "      context_class = tf.expand_dims(\n",
        "          tf.constant([context_word], dtype=\"int64\"), 1)\n",
        "      negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
        "          true_classes=context_class,\n",
        "          num_true=1, \n",
        "          num_sampled=num_ns, \n",
        "          unique=True, \n",
        "          range_max=vocab_size, \n",
        "          seed=SEED, \n",
        "          name=\"negative_sampling\")\n",
        "\n",
        "      # Build context and label vectors (for one target word)\n",
        "      negative_sampling_candidates = tf.expand_dims(\n",
        "          negative_sampling_candidates, 1)\n",
        "\n",
        "      context = tf.concat([context_class, negative_sampling_candidates], 0)\n",
        "      label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
        "\n",
        "      # Append each element from the training example to global lists.\n",
        "      targets.append(target_word)\n",
        "      contexts.append(context)\n",
        "      labels.append(label)\n",
        "\n",
        "  return targets, contexts, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XHiEpD0nwe0",
        "outputId": "5090285b-1687-4c10-a8f3-4a97800915b9"
      },
      "source": [
        "targets, contexts, labels = generate_training_data(\n",
        "    sequences=sequences, \n",
        "    window_size=2, \n",
        "    num_ns=4, \n",
        "    vocab_size=vocab_size, \n",
        "    seed=SEED)\n",
        "print(len(targets), len(contexts), len(labels))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 411525/411525 [02:51<00:00, 2404.10it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1617524 1617524 1617524\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIXitchpnwch",
        "outputId": "666d0fb9-5307-45d0-8a74-0e702e2afde1"
      },
      "source": [
        "BATCH_SIZE = 1024\n",
        "BUFFER_SIZE = 10000\n",
        "dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "print(dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<BatchDataset shapes: (((1024,), (1024, 5, 1)), (1024, 5)), types: ((tf.int32, tf.int64), tf.int64)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3S7bFbJnwaN",
        "outputId": "dec83a92-4e5c-45f0-a0f8-925ba809fccd"
      },
      "source": [
        "dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "print(dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<PrefetchDataset shapes: (((1024,), (1024, 5, 1)), (1024, 5)), types: ((tf.int32, tf.int64), tf.int64)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oi7DxXOzqcgW"
      },
      "source": [
        "\n",
        "# Set the number of negative samples per positive context. \n",
        "num_ns = 4\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mjyyvii2mxZR"
      },
      "source": [
        "class Word2Vec(Model):\n",
        "  def __init__(self, vocab_size, embedding_dim):\n",
        "    super(Word2Vec, self).__init__()\n",
        "    self.target_embedding = Embedding(vocab_size, \n",
        "                                      embedding_dim,\n",
        "                                      input_length=1,\n",
        "                                      name=\"w2v_embedding\", )\n",
        "    self.context_embedding = Embedding(vocab_size, \n",
        "                                       embedding_dim, \n",
        "                                       input_length=num_ns+1)\n",
        "    self.dots = Dot(axes=(3,2))\n",
        "    self.flatten = Flatten()\n",
        "\n",
        "  def call(self, pair):\n",
        "    target, context = pair\n",
        "    we = self.target_embedding(target)\n",
        "    ce = self.context_embedding(context)\n",
        "    dots = self.dots([ce, we])\n",
        "    return self.flatten(dots)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_zxr23fmxPA"
      },
      "source": [
        "def custom_loss(x_logit, y_true):\n",
        "      return tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=y_true)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0iRprfnpvdm"
      },
      "source": [
        "embedding_dim = 128\n",
        "word2vec = Word2Vec(vocab_size, embedding_dim)\n",
        "word2vec.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ax3dz60ppvbO"
      },
      "source": [
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1nGf_F1pvY7",
        "outputId": "0d3dd779-f5a0-458e-f99b-3e43572631b5"
      },
      "source": [
        "word2vec.fit(dataset, epochs=20, callbacks=[tensorboard_callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "1579/1579 [==============================] - 31s 19ms/step - loss: 1.3716 - accuracy: 0.4690\n",
            "Epoch 2/20\n",
            "1579/1579 [==============================] - 26s 16ms/step - loss: 1.1097 - accuracy: 0.5893\n",
            "Epoch 3/20\n",
            "1579/1579 [==============================] - 26s 16ms/step - loss: 0.9971 - accuracy: 0.6393\n",
            "Epoch 4/20\n",
            "1579/1579 [==============================] - 26s 17ms/step - loss: 0.9282 - accuracy: 0.6676\n",
            "Epoch 5/20\n",
            "1579/1579 [==============================] - 26s 16ms/step - loss: 0.8778 - accuracy: 0.6886\n",
            "Epoch 6/20\n",
            "1579/1579 [==============================] - 25s 16ms/step - loss: 0.8371 - accuracy: 0.7051\n",
            "Epoch 7/20\n",
            "1579/1579 [==============================] - 27s 17ms/step - loss: 0.8026 - accuracy: 0.7187\n",
            "Epoch 8/20\n",
            "1579/1579 [==============================] - 27s 17ms/step - loss: 0.7728 - accuracy: 0.7307\n",
            "Epoch 9/20\n",
            "1579/1579 [==============================] - 29s 18ms/step - loss: 0.7466 - accuracy: 0.7415\n",
            "Epoch 10/20\n",
            "1579/1579 [==============================] - 26s 17ms/step - loss: 0.7235 - accuracy: 0.7511\n",
            "Epoch 11/20\n",
            "1579/1579 [==============================] - 28s 18ms/step - loss: 0.7030 - accuracy: 0.7593\n",
            "Epoch 12/20\n",
            "1579/1579 [==============================] - 27s 17ms/step - loss: 0.6848 - accuracy: 0.7668\n",
            "Epoch 13/20\n",
            "1579/1579 [==============================] - 25s 16ms/step - loss: 0.6685 - accuracy: 0.7734\n",
            "Epoch 14/20\n",
            "1579/1579 [==============================] - 28s 18ms/step - loss: 0.6538 - accuracy: 0.7794\n",
            "Epoch 15/20\n",
            "1579/1579 [==============================] - 29s 18ms/step - loss: 0.6407 - accuracy: 0.7846\n",
            "Epoch 16/20\n",
            "1579/1579 [==============================] - 26s 17ms/step - loss: 0.6287 - accuracy: 0.7894\n",
            "Epoch 17/20\n",
            "1579/1579 [==============================] - 30s 19ms/step - loss: 0.6178 - accuracy: 0.7938\n",
            "Epoch 18/20\n",
            "1579/1579 [==============================] - 27s 17ms/step - loss: 0.6079 - accuracy: 0.7978\n",
            "Epoch 19/20\n",
            "1579/1579 [==============================] - 30s 19ms/step - loss: 0.5988 - accuracy: 0.8015\n",
            "Epoch 20/20\n",
            "1579/1579 [==============================] - 28s 18ms/step - loss: 0.5904 - accuracy: 0.8048\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7feb533412d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    }
  ]
}
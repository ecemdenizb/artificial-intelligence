{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "imdb-movie-sentiment.ipynb ",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4t5E-pL-xpL"
      },
      "source": [
        "!wget https://www.dropbox.com/s/fz2d3s2ngq8aw2e/turkish_movie_sentiment_dataset.csv?dl=1 -O dataset.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_glIGL0dBUJW"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "from tensorflow.keras.models import Sequential, load_model\r\n",
        "from tensorflow.keras.layers import Dense, GRU, Embedding, Dropout, SimpleRNN, Conv1D\r\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\r\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\r\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBH34B9fBvkt"
      },
      "source": [
        "dataset = pd.read_csv('dataset.csv')\r\n",
        "dataset.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AirQokLxdcVx"
      },
      "source": [
        "#Ã¶zge\r\n",
        "def convert(point):\r\n",
        "  p = point.split(',')\r\n",
        "  a = int(p[0])*10 + int(p[1])\r\n",
        "  if a >= 25:\r\n",
        "    return 1\r\n",
        "  else:\r\n",
        "    return 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITeMX_O5bo2d"
      },
      "source": [
        "data_point =dataset['point'].apply(convert)\r\n",
        "target = data_point.values.tolist()\r\n",
        "\r\n",
        "data = dataset['comment'].values.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VetSH3Pbbo0f"
      },
      "source": [
        "cutoff = int(len(data) * 0.8)\r\n",
        "x_train, x_test = data[:cutoff], data[cutoff:]\r\n",
        "y_train, y_test = target[:cutoff], target[cutoff:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFSzAZo6boxu"
      },
      "source": [
        "num_words = 10000\r\n",
        "tokenizer = Tokenizer(num_words=num_words)\r\n",
        "tokenizer.fit_on_texts(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWRAXkembotO"
      },
      "source": [
        "x_train_tokens = tokenizer.texts_to_sequences(x_train)\r\n",
        "x_train[100], x_train_tokens[100]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06bbCavMVXv3"
      },
      "source": [
        "x_test_tokens = tokenizer.texts_to_sequences(x_test)\r\n",
        "x_test[100], x_test_tokens[100]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxoZY0_FVXtr"
      },
      "source": [
        "num_tokens = [len(tokens) for tokens in x_train_tokens + x_test_tokens]\r\n",
        "num_tokens = np.array(num_tokens)\r\n",
        "np.mean(num_tokens), np.max(num_tokens), np.argmax(num_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3RCsQ4CVXrH"
      },
      "source": [
        "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\r\n",
        "max_tokens = int(max_tokens)\r\n",
        "max_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICUjw57tVXon"
      },
      "source": [
        "np.sum(num_tokens < max_tokens) / len(num_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBI1wS86VXl2"
      },
      "source": [
        "x_train_pad = pad_sequences(x_train_tokens, maxlen=max_tokens)\r\n",
        "x_test_pad = pad_sequences(x_test_tokens, maxlen=max_tokens)\r\n",
        "x_train_pad.shape, x_test_pad.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NA8UpKMFVXiJ"
      },
      "source": [
        "idx = tokenizer.word_index\r\n",
        "inverse_map = dict(zip(idx.values(), idx.keys()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orkCVpjXboq9"
      },
      "source": [
        "def tokens_to_string(tokens):\r\n",
        "    words = [inverse_map[token] for token in tokens if token!=0]\r\n",
        "    text = ' '.join(words)\r\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmSF43r5V4UL"
      },
      "source": [
        "x_train_tokens[100], tokens_to_string(x_train_tokens[100])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKPsMzBmV4R6"
      },
      "source": [
        "embedding_size = 35\r\n",
        "model = Sequential()\r\n",
        "\r\n",
        "model.add(Embedding(input_dim=num_words,\r\n",
        "                    output_dim=embedding_size,\r\n",
        "                    input_length=max_tokens,\r\n",
        "                    name='embedding_layer'))\r\n",
        "\r\n",
        "\r\n",
        "model.add(Conv1D(filters=64, kernel_size=(2), padding=\"same\", activation=None))\r\n",
        "model.add(tf.keras.layers.LeakyReLU())\r\n",
        "model.add(GRU(units=32, return_sequences=True))\r\n",
        "model.add(Conv1D(filters=32, kernel_size=(2), padding=\"same\", activation=None))\r\n",
        "model.add(tf.keras.layers.LeakyReLU())\r\n",
        "model.add(GRU(units=16, return_sequences=True))\r\n",
        "model.add(Conv1D(filters=16, kernel_size=(2), padding=\"same\", activation=None))\r\n",
        "model.add(tf.keras.layers.LeakyReLU())\r\n",
        "model.add(GRU(units=8, return_sequences=True))\r\n",
        "model.add(Conv1D(filters=8, kernel_size=(2), padding=\"same\", activation=None))\r\n",
        "model.add(tf.keras.layers.LeakyReLU())\r\n",
        "model.add(GRU(units=4))\r\n",
        "model.add(Dense(16, activation='relu'))\r\n",
        "model.add(Dense(4, activation='relu'))\r\n",
        "\r\n",
        "model.add(Dense(1))\r\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tz_aW-9IV4PK"
      },
      "source": [
        "optimizer = RMSprop(lr=1e-3)\r\n",
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\r\n",
        "              optimizer=optimizer,\r\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XswfO48OV4Me"
      },
      "source": [
        "from tensorflow.data import Dataset\r\n",
        "train_ds = Dataset.zip((Dataset.from_tensor_slices(x_train_pad), Dataset.from_tensor_slices(y_train)))\r\n",
        "train_ds = train_ds.shuffle(1024).batch(256)\r\n",
        "val_ds = Dataset.zip((Dataset.from_tensor_slices(x_test_pad), Dataset.from_tensor_slices(y_test)))\r\n",
        "val_ds = val_ds.batch(256)\r\n",
        "steps_per_epoch = len(x_train_pad) // 256\r\n",
        "validation_steps = len(x_test_pad) // 256"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5sVGPjAvoru"
      },
      "source": [
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', factor=0.2,patience=1, min_lr=1e-7, mode='max')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvYX2q9sV4Hn"
      },
      "source": [
        "model.fit(train_ds.repeat(), epochs=5, steps_per_epoch=steps_per_epoch, validation_data=val_ds.repeat(), validation_steps=validation_steps, callbacks=[reduce_lr])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVVmkxXiV4Ao"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}